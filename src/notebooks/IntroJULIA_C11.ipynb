{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Programación en JULIA\n",
    "## Notebook 11\n",
    "\n",
    "Mauricio Tejada\n",
    "\n",
    "ILADES - Universidad Alberto Hurtado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenidos\n",
    "\n",
    "- [Optimización](#11.-Optimización)\n",
    "    - [Métodos Numéricos de Optimización: Ideas Básicas](#11.1-Métodos-Numéricos-de-Optimización:-Ideas-Básicas)\n",
    "    - [Paquetes de Optimización de Julia (Optim y JuMP)](#11.2-Paquetes-de-Optimizaci%C3%B3n-de-Julia-(Optim-y-JuMP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optimización\n",
    "\n",
    "*La exposición en esta sección sigue de cerca el capítulo 4 del libro de Miranda y Fackler.*\n",
    "\n",
    "En esta sección se describe cómo maximizar numéricamente una función respecto de un vector finito de variables. El problema a resolver es entonces:\n",
    "\n",
    "$$\\max_{x \\in X} f(x)$$\n",
    "\n",
    "con $x \\subseteq R^n$. En este problemas denominamos a $f$ como la función objetivos, a $X$ como el conjunto factible y a $x^*$ como el máximo (si existe).\n",
    "\n",
    "Los problemas de optimización finitos son muy comunes en economía.\n",
    "\n",
    "*Teorema de Wierstrass**\n",
    "\n",
    "Si $f$ es continua y $X$ es un conjunto no vacío, cerrado y acotado, entonces $f$ tiene un máximo en $X$. \n",
    "\n",
    "Condición de Primer Orden:\n",
    "\n",
    "$$f'(x^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Métodos Numéricos de Optimización: Ideas Básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Método de Newton-Rapson\n",
    "\n",
    "El método de Newton-Rapson usa una sucesión de aproximaciones cuadráticas de la función objetivo y el máximo de la aproximación debería converger al máximo de la función. \n",
    "\n",
    "Este método está estrechamente relacionado con el método de Newton para buscar las raíces de una función.\n",
    "\n",
    "El método es iterativo y se inicia con una conjetura $x^{(0)}$ de la solución. \n",
    "\n",
    "A continuación se actualiza la solución de $x^{(k)}$ a $x^{(k+1)}$ minimizando:\n",
    "\n",
    "$$f(x) \\approx f(x^{(k)}) + f'(x^{(k)})(x-x^{(k)}) + 0.5 (x-x^{(k)})^{T}f''(x^{(k)})(x-x^{(k)})$$\n",
    "\n",
    "respecto de $x$. La condición de primero orden es:\n",
    "\n",
    "$$f'(x^{(k)}) + f''(x^{(k)})(x-x^{(k)})=0$$\n",
    "\n",
    "y por tanto:\n",
    "\n",
    "$$x^{(k+1)} \\leftarrow x^{(k)} - [f''(x^{(k)})]^{-1} f'(x^{(k)})$$\n",
    "\n",
    "El algoritmo termina cuando $x^{(k+1)}$ es suficientemente cercano a $x^{(k)}$. \n",
    "\n",
    "\n",
    "Limitaciones:\n",
    "- Se requiere computar tanto la primera como la segunda derivadas de la función (el gradiente y la matriz hessiana).\n",
    "- No hay garantía que la función se incremente en la dirección de la actualización (esto se garantiza sólo si $f''$ es definida negativa).\n",
    "- EL método de Newton-Rapson es usada sólo cuando la función es globalmente cóncava."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Método de Quasi-Newton\n",
    "\n",
    "Sigue la misma idea que el método de Newton-Rapson pero reemplaza la matriz hessiana con una aproximación que es definida negativa (garantizando incrementos en la función).\n",
    "\n",
    "Por eficiencia, la aproximación se realiza directamente sobre la inversa de la matriz hessiana. El método Broyden-Fletcher-Goldfarb-Shano (BFGS) satisface esta idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Método Nelder-Mead\n",
    "\n",
    "El objetivo es encontrar el *simplex* que contiene el punto que minimiza (localmente) la función. Este método no requiere derivadas y va buscando el mínimo iterativamente mediante un proceso de reemplazo de puntos. Si el problema es minimizar una función $f(x): R^N \\rightarrow R$ entonces el simplex tendrá $n+1$ vértices. El método evaluará la función en cada uno de estos $n+1$ vértices y reemplazará el peor valor de la función por una nueva conjetura. Este proceso se realiza en 4 pasos que son denominados (1) reflejo, (2) expansión, (3) contracción, y (4) encoger. Por ejemplo, si suponemos que buscamos el mínimo de la función $f(x): R^2 \\rightarrow R$, entonces el simplex tendrá 3 vértices (un triángulo) y en cada paso de la iteración se reemplazará el peor valor de la función por un nuevo vértice del triángulo. Este proceso mueve el triángulo siempre hacia abajo en dirección de la función. Los siguientes diagramas disponibles en [Wikipedia](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) muestran gráficamente este proceso:\n",
    "\n",
    "![](figs/Nelder-Mead_Rosenbrock.gif)\n",
    "![](figs/Nelder-Mead_Himmelblau.gif)\n",
    "<center>Fuente: Wikipedia</center>\n",
    "\n",
    "Este es un método bien popular cuando las derivadas no son bien comportadas y es relativamente rápido (aunque en general requiere de un gran número de evaluaciones de la función)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Paquetes de Optimización de Julia (Optim y JuMP)\n",
    "\n",
    "El paquete Optim implementa varios procedimientos de optimización. En particular, el paquete busca $x$ en $R^n$ de tal manera de *minimizar* la función $f(x):R^n \\rightarrow R$. El paquete incluye procedimientos de optimización como los descritos antes y se centra principalmente en problemas no restringidos. Optim tiene algún soporte para optimización restringida, pero este es únicamente para restricciones en los límites de las variables de elección. \n",
    "\n",
    "**Nota:** Optim tiene implementados los métodos numéricos anteriores con el objetivo de minimizar una función. La maximización se realiza simplemente minimizando el negativo de la función objetivos.\n",
    "\n",
    "#### Minimización No Restringida usando Optim\n",
    "\n",
    "Julia minimiza funciones no lineales sin restricciones usando la función `optimize()`. \n",
    "\n",
    "La función busca iterativamente el mínimo de una función escalar con varias variables a partir de una conjetura inicial. \n",
    "\n",
    "La sintaxis general es:\n",
    "\n",
    "`resultado = optimize(fun, gfun, hfun, x0, método, otras_opciones)`\n",
    "\n",
    "Los inputs de la función son:\n",
    "\n",
    "- `fun`: Función escalar objetivo. \n",
    "- `gfun`: Gradiente (opcional)\n",
    "- `hfun`: Matriz Hessiana (opcional)\n",
    "- `x0`: Conjetura inicial.\n",
    "- `métodos`: Método de optimización (entre otros Newton, BFGS, Nelder Mead).\n",
    "- `otras_opciones`: opciones de optimización (número de iteraciones, tolerancia, mostrar iteraciones, entre otros)\n",
    "\n",
    "\n",
    "El método de optimización a usarse depende del problema. Si el gradiente `gfun` y la hessiana `hfun` están disponibles se usa el método de `Newton()`. Si el el gradiente y la hessiana no está disponibles, los métodos de Quasi-Newton son usados: `BFGS()` o `LBFGS()`. Finalmente, si la función no es diferenciable se usa el método de `Nelder-Mead()`. \n",
    "\n",
    "Los resultados de interés se encuentran en:\n",
    "\n",
    "- `resultado.minimizer`: $x^*$ que minimiza la función.\n",
    "- `resultado.minimum`: función evaluada en el óptimo $f(x^*)$.\n",
    "\n",
    "Resolvamos el siguiente ejemplo:\n",
    "\n",
    "$$f(x)=3x^2_1 +2x_1 x_2 +x^2_2 -4x_1 + 5x_2$$\n",
    "\n",
    "Escribamos la función anónima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "funx (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funx(x) = 3*x[1]^2 + 2*x[1]*x[2] + x[2]^2 - 4*x[1] + 5*x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hallar el mínimo partimos de la conjetura $[1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [1.0, 1.0]\n",
    "res_min = optimize(funx, x0, BFGS());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       "  2.2499999999546794\n",
       " -4.749999999937206"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_min.minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_min.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aproximar las derivadas `optimize()` usa por defecto el método de diferencia finitas. No obstante, es posible usar alternativamente *Diferenciación Automática*, para ello se usa la opción `autodiff = :forward`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       "  2.25\n",
       " -4.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_min = optimize(funx, x0, BFGS(), autodiff = :forward)\n",
    "res_min.minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de las opciones se declaran bajo `Optim.Options()` y las más utilizadas son:\n",
    "\n",
    "- `x_tol`: Tolerancia relativa a cambios en el vector x (por defecto 0.0).\n",
    "- `f_tol`: Tolerancia relativa a cambios en la función objetivo (por defecto 0.0).\n",
    "- `f_calls_limit`: Número máximo de evaluaciones de la función. (por defecto 0, que significa ilimitado).\n",
    "- `iterations`: Máximo número de iteraciones (por defecto 1000).\n",
    "- `show_trace`: Mostrar resultados de iteraciones (por defecto false).\n",
    "\n",
    "Usemos ahora el método de Nelder Mead (Simplex) mostrando las iteraciones y restringiendo a que el máximo número de iteraciones sea 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value    √(Σ(yᵢ-ȳ)²)/n \n",
      "------   --------------    --------------\n",
      "     0     7.000000e+00     2.051376e+00\n",
      " * time: 0.02609419822692871\n",
      "     1     7.000000e+00     3.268771e+00\n",
      " * time: 0.11155319213867188\n",
      "     2     2.009219e+00     4.721712e+00\n",
      " * time: 0.11174821853637695\n",
      "     3    -4.531133e+00     5.965001e+00\n",
      " * time: 0.11193299293518066\n",
      "     4    -1.257614e+01     4.552313e+00\n",
      " * time: 0.11211514472961426\n",
      "     5    -1.524052e+01     1.250000e+00\n",
      " * time: 0.1233220100402832\n",
      "     6    -1.524052e+01     3.248085e-01\n",
      " * time: 0.12361311912536621\n",
      "     7    -1.591637e+01     4.124580e-01\n",
      " * time: 0.12384700775146484\n",
      "     8    -1.622880e+01     1.601745e-01\n",
      " * time: 0.12406802177429199\n",
      "     9    -1.627810e+01     2.038907e-02\n",
      " * time: 0.12428903579711914\n",
      "    10    -1.627810e+01     3.624890e-02\n",
      " * time: 0.1245121955871582\n",
      "    11    -1.634458e+01     3.331949e-02\n",
      " * time: 0.12472319602966309\n",
      "    12    -1.635235e+01     8.676904e-03\n",
      " * time: 0.12496805191040039\n",
      "    13    -1.636560e+01     6.088693e-03\n",
      " * time: 0.1251811981201172\n",
      "    14    -1.636560e+01     2.835991e-04\n",
      " * time: 0.12543416023254395\n",
      "    15    -1.636560e+01     4.211148e-03\n",
      " * time: 0.12564921379089355\n",
      "    16    -1.637430e+01     4.048255e-03\n",
      " * time: 0.12586522102355957\n",
      "    17    -1.637430e+01     3.466096e-03\n",
      " * time: 0.12607908248901367\n",
      "    18    -1.637430e+01     1.526553e-03\n",
      " * time: 0.1263902187347412\n",
      "    19    -1.637430e+01     8.710935e-04\n",
      " * time: 0.12663006782531738\n",
      "    20    -1.637430e+01     5.071956e-04\n",
      " * time: 0.12684106826782227\n",
      "    21    -1.637430e+01     5.770953e-04\n",
      " * time: 0.12705111503601074\n",
      "    22    -1.637477e+01     2.000483e-04\n",
      " * time: 0.1273031234741211\n",
      "    23    -1.637477e+01     8.453215e-05\n",
      " * time: 0.12752509117126465\n",
      "    24    -1.637485e+01     6.297472e-05\n",
      " * time: 0.127730131149292\n",
      "    25    -1.637492e+01     5.722893e-05\n",
      " * time: 0.12798118591308594\n",
      "    26    -1.637499e+01     2.701253e-05\n",
      " * time: 0.12816309928894043\n",
      "    27    -1.637499e+01     1.711999e-05\n",
      " * time: 0.1283421516418457\n",
      "    28    -1.637499e+01     5.082894e-06\n",
      " * time: 0.12851905822753906\n",
      "    29    -1.637500e+01     3.122085e-06\n",
      " * time: 0.12870216369628906\n",
      "    30    -1.637500e+01     1.177134e-06\n",
      " * time: 0.12888121604919434\n",
      "    31    -1.637500e+01     1.402858e-06\n",
      " * time: 0.12906122207641602\n",
      "    32    -1.637500e+01     6.690414e-07\n",
      " * time: 0.12931203842163086\n",
      "    33    -1.637500e+01     1.975975e-07\n",
      " * time: 0.12959718704223633\n",
      "    34    -1.637500e+01     1.333604e-07\n",
      " * time: 0.1298351287841797\n",
      "    35    -1.637500e+01     6.865298e-08\n",
      " * time: 0.1300501823425293\n",
      "    36    -1.637500e+01     2.305463e-08\n",
      " * time: 0.13029003143310547\n",
      "    37    -1.637500e+01     1.510244e-08\n",
      " * time: 0.1305091381072998\n",
      "    38    -1.637500e+01     1.564809e-08\n",
      " * time: 0.1307201385498047\n",
      "    39    -1.637500e+01     1.317066e-09\n",
      " * time: 0.13092708587646484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       "  2.250005459308121\n",
       " -4.750017444233452"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_min = optimize(funx, x0, NelderMead(), Optim.Options(show_trace = true, iterations = 50));\n",
    "res_min.minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el método de Newton es necesario proveer explícitamente el gradiente y la hessiana de la función. En el caso de la función que tomamos como ejemplo tenemos:\n",
    "\n",
    "- El gradiente es:\n",
    "\n",
    "$$\\left(\\frac{\\partial f(x_1,x_2)}{\\partial x_1}, \\frac{\\partial f(x_1,x_2)}{\\partial x_2}\\right) = \\left(6 x_1+2x_2−4, 2x_1+2x_2+5 \\right) $$\n",
    "\n",
    "- La Hessiana es:\n",
    "$$\n",
    "\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}f(x_{1},x_{2})}{\\partial x_{1}\\partial x_{\\text{1}}} & \\frac{\\partial^{2}f(x_{1},x_{2})}{\\partial x_{1}\\partial x_{2}}\\\\\n",
    "\\frac{\\partial^{2}f(x_{1},x_{2})}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2}f(x_{1},x_{2})}{\\partial x_{2}\\partial x_{2}}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{cc}\n",
    "6 & 2 \\\\\n",
    "2 & 2\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Creamos dos funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hfunx (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gfunx(x)\n",
    "    g = zeros(2)\n",
    "    \n",
    "    g[1] = 6*x[1] + 2*x[2] - 4\n",
    "    g[2] = 2*x[1] + 2*x[2] + 5\n",
    "    \n",
    "    return g\n",
    "end\n",
    "\n",
    "function hfunx(x)\n",
    "    h = zeros(2,2)\n",
    "    \n",
    "    h[1,1] = 6\n",
    "    h[1,2] = 2\n",
    "    h[2,1] = 2\n",
    "    h[2,2] = 2\n",
    "    \n",
    "    return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       "  2.25\n",
       " -4.75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_min = optimize(funx, gfunx, hfunx, x0, Newton(), inplace = false)\n",
    "res_min.minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimización Restringida usando Optim\n",
    "\n",
    "\n",
    "Optim permite resolver problemas restringidos simples en donde las restricciones toman la forma de límites para las variables de elección. En particular, permite resolver el siguiente problema: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min_{x} && f(x) \\\\\n",
    "s.a &&  \\\\\n",
    "&& lb \\leq x \\leq ub \n",
    "\\end{eqnarray}\n",
    "\n",
    "donde $lb$ y $ub$ son vectores de la misma dimensión que $x$.\n",
    "\n",
    "La sintaxis general es:\n",
    "\n",
    "`resultado = optimize(fun, gfun, hfun, lb, ub, x0, Fminbox(método), otras_opciones)`\n",
    "\n",
    "Los nuevos inputs de la función son:\n",
    "\n",
    "- `lb`: Vector con límites inferiores (`-Inf` es posible). \n",
    "- `ub`: Vector con límites superiores (`Inf` es posible). \n",
    "- `Fminbox()`: Declara optimización restringida.\n",
    "\n",
    "Ahora resolvamos el siguiente problema:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min_{x_1,x_2}&& f(x_1,x_2) = 1+\\frac{x_1}{(1+x_2)} - 3x_1x_2 + x_2(1+x_1) \\\\\n",
    "s.a &&  \\\\\n",
    "&&  0 \\leq x_1 \\leq 1\\\\\n",
    "&&  0 \\leq x_2 \\leq 2\\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunObj (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FunObj(x) = 1 + x[1]/(1+x[2]) - 3*x[1]*x[2] + x[2]*(1+x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = [0.0, 0.0]\n",
    "ub = [1.0, 2.0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.9999999999999999\n",
       " 1.9999999999999998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = [0.5, 1.0]\n",
    "resultados = optimize(FunObj, lb, ub, x0, Fminbox(LBFGS()))\n",
    "resultados.minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización usado JuMP\n",
    "\n",
    "**JuMP** es un lenguaje de optimización matemática escrito en Julia similar a algunos programas comerciales como GAMS. Entre sus principales características están su facilidad para trasladar un problema matemático en uno computacional y que permite utilizar *solver* externos (tanto open source como comerciales). De acuerdo a la documentación de JuMP, los solvers que éste soporta incluyen: Artelys Knitro, Bonmin, Cbc, Clp, Couenne, CPLEX, ECOS, FICO Xpress, GLPK, Gurobi, Ipopt, MOSEK, NLopt, y SCS. Aquí se presenta JuMP usando algunos ejemplos y algunas de sus capacidades básicas, para un tratamiento más profundo ver su [documentación](http://www.juliaopt.org/JuMP.jl/stable/).  \n",
    "\n",
    "El problema general es encontrar $x$ que minimiza la función no lineal objetivo $f(x)$ (lineal o no lineal) sujeto a restricciones, que pueden ser lineales o no lineales y pueden de igualdad o de desigualdad.\n",
    "\n",
    "El problema general es:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min_{x} && f(x) \\\\\n",
    "s.a &&  \\\\\n",
    "&& Ax = b \\\\\n",
    "&& C(x) = 0  \\\\\n",
    "&& AIx \\leq bI \\\\\n",
    "&& CI(x) = 0 \\\\\n",
    "&& lb \\leq x \\leq ub \n",
    "\\end{eqnarray}\n",
    "\n",
    "donde $A$ y $AI$ son matrices, $C(x)$ y $CI(x)$ son funciones no lineales, $b$ y $bI$ son vectores columna. La dimensiones de estos objetos dependen del número de restricciones. Finalmente, $lb$ y $ub$ son vectores columnas de la misma dimensión que $x$.\n",
    "\n",
    "Las opciones básicas de JuMP para declara un modelo de optimización y resolverlo son:\n",
    "\n",
    "- `nombre_modelo = Model(optimizador)`: Declara el modelo (vacío) usando un optimizador particular.\n",
    "- `@variable(nombre_modelo, li <= nombre_variable <= ls, tipo variable)`: Declara variable con el nombre `nombre_variable`, las restricciones dada por los límites `[li,ls]`, y el tipo de variable (opciones: `Bin` para binarias e `Int` para enteros). \n",
    "- `@constraint(nombre_modelo, nombre_restriccion, expresión)`: Declara restricciones lineales definidas por `expresión`, las mismas que pueden ser de igualdad `==` o desigualdad `<=,>=`.\n",
    "- `@NLconstraint(nombre_modelo, nombre_restriccion, expresión)`: Declara restricciones no lineales definidas por `expresión`, las mismas que pueden ser de igualdad `==` o desigualdad `<=,>=`.\n",
    "- `@objective(nombre_modelo, [Min/Max], expresión)`: Declara la función objetivo lineal. \n",
    "- `@NLobjective(nombre_modelo, [Min/Max], expresión)`: Declara la función objetivo no lineal. \n",
    "- `print(nombre_modelo)`: Imprime en pantalla el modelo en formato legible para humanos.\n",
    "- `optimize!(nombre_modelo)`: Resuelve el modelo declarado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplo resolvamos el siguiente problema de optimización lineal trivial:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{minimize} \\qquad && 2x+3y \\\\\n",
    " \\text{subject to} \\quad \\quad && x+y \\leq 1 \\\\\n",
    " \\qquad \\qquad && x \\geq 0.1, y \\geq 0.1 \\\\\n",
    " \\qquad \\qquad && x,y \\in \\mathbb{R}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Cargamos `JuMP`y el optimizador (en este caso `GLPK`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using GLPK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El problema de optimización es:\n",
      "Min 2 x + 3 y\n",
      "Subject to\n",
      " x + y ≤ 1.0\n",
      " x ≥ 0.1\n",
      " y ≥ 0.1\n"
     ]
    }
   ],
   "source": [
    "mod_ejemplo = Model(GLPK.Optimizer)\n",
    "\n",
    "@variable(mod_ejemplo, x >= 0.1)\n",
    "@variable(mod_ejemplo, y >= 0.1)\n",
    "@constraint(mod_ejemplo, x + y <= 1)\n",
    "@objective(mod_ejemplo, Min, 2*x + 3*y)\n",
    "\n",
    "println(\"El problema de optimización es:\")\n",
    "print(mod_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolvemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize!(mod_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son (usamos las funciones `JuMP.objective_value` y `JuMP.value` para obtener la función objetivo y los valores de $x$ e $y$ que minimizan la función):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función Objetivo: 0.5\n",
      "x = 0.1\n",
      "y = 0.1\n"
     ]
    }
   ],
   "source": [
    "println(\"Función Objetivo: \", JuMP.objective_value(mod_ejemplo))\n",
    "println(\"x = \", JuMP.value(x)) \n",
    "println(\"y = \", JuMP.value(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JuMP resuelve problema de programación lineal generales del tipo:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "& \\text{minimize} && c^T x \\\\\n",
    "& \\text{subject to} && A x \\leq b \\\\\n",
    "&                   && x \\geq 0 \\\\\n",
    "&                   && x \\in \\mathbb{R}^n\n",
    "\\end{eqnarray}\n",
    "\n",
    "Un ejemplo de problema de minimización de elección de portafolio minimizando el riesgo:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "& \\text{minimize} && RiesgoTotal = \\sum_i \\sigma_i x_i \\\\\n",
    "& \\text{subject to} && \\sum_i x_i (R_i - R_{min}) \\geq 0 \\\\\n",
    "&                   && \\sum_i x_i (PV_i - PV_{min}) \\geq 0 \\\\\n",
    "&                   &&  0 \\leq x_i \\leq 1 \\forall i\\\\\n",
    "&                   && \\sum_i x_i = 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "Supongamos que obtenemos información bursátil de 7 empresas (riesgo, rentabilidad, y precio): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentabilidad = [0.01130, 0.02565, 0.00440, 0.00972, 0.01259, 0.00652, 0.00681]\n",
    "riesgo = [0.07752, 0.06172, 0.10399, 0.08940, 0.06157, 0.09916, 0.06297]\n",
    "precio = [5.89, 28.03, 11.77, 6.6, 56.05, 70.11, 13.47]\n",
    "rentabilidadmin = 0 # Rentabilidad mínima\n",
    "preciomin = 0       # Precio mínimo\n",
    "n = 7;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos el modelo de la misma forma que antes (la única diferencia es que ahora usamos `x[1:7]` para declarar las 7 variables de una sola vez y usamos *list comprehensions* para declarar las restricciones y la función objetivo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El problema de optimización es:\n",
      "Min 0.07752 x[1] + 0.06172 x[2] + 0.10399 x[3] + 0.0894 x[4] + 0.06157 x[5] + 0.09916 x[6] + 0.06297 x[7]\n",
      "Subject to\n",
      " c1 : x[1] + x[2] + x[3] + x[4] + x[5] + x[6] + x[7] = 1.0\n",
      " c2 : 0.0113 x[1] + 0.02565 x[2] + 0.0044 x[3] + 0.00972 x[4] + 0.01259 x[5] + 0.00652 x[6] + 0.00681 x[7] ≥ 0.0\n",
      " c3 : 5.89 x[1] + 28.03 x[2] + 11.77 x[3] + 6.6 x[4] + 56.05 x[5] + 70.11 x[6] + 13.47 x[7] ≥ 0.0\n",
      " x[1] ≥ 0.0\n",
      " x[2] ≥ 0.0\n",
      " x[3] ≥ 0.0\n",
      " x[4] ≥ 0.0\n",
      " x[5] ≥ 0.0\n",
      " x[6] ≥ 0.0\n",
      " x[7] ≥ 0.0\n",
      " x[1] ≤ 1.0\n",
      " x[2] ≤ 1.0\n",
      " x[3] ≤ 1.0\n",
      " x[4] ≤ 1.0\n",
      " x[5] ≤ 1.0\n",
      " x[6] ≤ 1.0\n",
      " x[7] ≤ 1.0\n"
     ]
    }
   ],
   "source": [
    "portafolio = Model(GLPK.Optimizer);\n",
    "@variable(portafolio, 0 <= x[1:n] <= 1)\n",
    "@constraint(portafolio, c1,  sum(x[1:n]) == 1)\n",
    "@constraint(portafolio, c2,  sum([x[i]*(rentabilidad[i]-rentabilidadmin) for i in 1:n]) >= 0)\n",
    "@constraint(portafolio, c3,  sum([x[i]*(precio[i]-preciomin) for i in 1:n]) >= 0)\n",
    "@objective(portafolio, Min, sum([riesgo[i]*x[i] for i in 1:n]))\n",
    "println(\"El problema de optimización es:\")\n",
    "print(portafolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolvemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize!(portafolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función Objetivo: 0.06157\n",
      "x1 = 0.0\n",
      "x2 = 0.0\n",
      "x3 = 0.0\n",
      "x4 = 0.0\n",
      "x5 = 1.0\n",
      "x6 = 0.0\n",
      "x7 = 0.0\n"
     ]
    }
   ],
   "source": [
    "println(\"Función Objetivo: \", JuMP.objective_value(portafolio))\n",
    "for i in 1:n\n",
    "    println(\"x$i = \", JuMP.value(x[i])) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estrategia óptima entonces consiste en invertir todo en una sola empresa, la número 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora resolvamos un modelo no lineal. A modo de ejemplo usamos la función de Rosenbrock como función objetivo y resolvamos el siguiente problema:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min_{x_1,x_2} &&  f(x_1,x_2) =  100(x_2-x^2_1)^2 + (1-x_1)^2 \\\\\n",
    "s.a &&  \\\\\n",
    "&& x_1 + 2x_2 \\leq 1\\\\\n",
    "&& 2x_1 + x_2= 1  \n",
    "\\end{eqnarray}\n",
    "\n",
    "Cargamos ahora un optimizador que sea capaz de resolver problemas no lineales (en este caso `Ipopt`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Ipopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El problema de optimización es:\n",
      "Min 100.0 * (x[2] - x[1] ^ 2.0) ^ 2.0 + (1.0 - x[1]) ^ 2.0\n",
      "Subject to\n",
      " ceq : 2 x[1] + x[2] = 1.0\n",
      " cineq : x[1] + 2 x[2] ≤ 1.0\n"
     ]
    }
   ],
   "source": [
    "nl_mod_ejemplo = Model(Ipopt.Optimizer);\n",
    "\n",
    "@variable(nl_mod_ejemplo, x[1:2])\n",
    "\n",
    "x0 = [0.5, 0]\n",
    "A = [1, 2]\n",
    "b = 1\n",
    "Aeq = [2, 1]\n",
    "beq = 1\n",
    "\n",
    "@constraint(nl_mod_ejemplo, ceq, sum(Aeq.*x) == beq)\n",
    "@constraint(nl_mod_ejemplo, cineq, sum(A.*x) <= b)\n",
    "\n",
    "@NLobjective(nl_mod_ejemplo, Min, 100*(x[2]-x[1]^2)^2 + (1-x[1])^2)\n",
    "\n",
    "println(\"El problema de optimización es:\")\n",
    "print(nl_mod_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolvemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.13.2, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        2\n",
      "Number of nonzeros in inequality constraint Jacobian.:        2\n",
      "Number of nonzeros in Lagrangian Hessian.............:        3\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        1\n",
      "Total number of inequality constraints...............:        1\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.0000000e+00 1.00e+00 1.07e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  7.0257866e+00 0.00e+00 5.24e+01  -1.0 5.03e-01    -  1.00e+00 1.00e+00h  1\n",
      "   2  3.9411734e-01 0.00e+00 8.90e+00  -1.0 2.42e-01    -  1.00e+00 1.00e+00f  1\n",
      "   3  3.4325787e-01 0.00e+00 6.69e-02  -1.0 2.15e-02    -  1.00e+00 1.00e+00f  1\n",
      "   4  3.4271827e-01 0.00e+00 8.13e-04  -2.5 2.37e-03    -  1.00e+00 1.00e+00f  1\n",
      "   5  3.4271758e-01 0.00e+00 1.04e-06  -3.8 8.47e-05    -  1.00e+00 1.00e+00f  1\n",
      "   6  3.4271757e-01 0.00e+00 1.70e-09  -5.7 3.44e-06    -  1.00e+00 1.00e+00f  1\n",
      "   7  3.4271757e-01 0.00e+00 2.59e-13  -8.6 4.23e-08    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 7\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   3.4271757484331294e-01    3.4271757484331294e-01\n",
      "Dual infeasibility......:   2.5923707624997405e-13    2.5923707624997405e-13\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5062219588631739e-09    2.5062219588631739e-09\n",
      "Overall NLP error.......:   2.5062219588631739e-09    2.5062219588631739e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 8\n",
      "Number of objective gradient evaluations             = 8\n",
      "Number of equality constraint evaluations            = 8\n",
      "Number of inequality constraint evaluations          = 8\n",
      "Number of equality constraint Jacobian evaluations   = 1\n",
      "Number of inequality constraint Jacobian evaluations = 1\n",
      "Number of Lagrangian Hessian evaluations             = 7\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      1.314\n",
      "Total CPU secs in NLP function evaluations           =      0.905\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    }
   ],
   "source": [
    "optimize!(nl_mod_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solución es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función Objetivo: 0.34271757484331294\n",
      "x1 = 0.4149443155076643\n",
      "x2 = 0.1701113689846714\n"
     ]
    }
   ],
   "source": [
    "println(\"Función Objetivo: \", JuMP.objective_value(nl_mod_ejemplo))\n",
    "println(\"x1 = \", JuMP.value(x[1])) \n",
    "println(\"x2 = \", JuMP.value(x[2])) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
